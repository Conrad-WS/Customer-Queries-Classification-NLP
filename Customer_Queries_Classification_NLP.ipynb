{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Customer-Queries-Classification-NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/busyML/Customer-Queries-Classification-NLP/blob/master/Customer_Queries_Classification_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "i09FSfT-M_Yf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Say What? Customer Query Classification with  some simple A.I\n",
        "\n",
        "![alt text](https://blog.salemove.com/wp-content/uploads/2018/01/AICSR.jpg)\n",
        "\n",
        "I´m happy to welcome you back to yet another short tutorial on how to use Machine Learning to help us accelerate and facilitate things in your company. Whatever sector you might be in, I´m certain that you have clients. And clients have this pesky tendency of complaining or requesting help...\n",
        "\n",
        "Today´s exercise will show you how we can take a written message from a customer requesting customer support and classify that message as we see fit. The messages I am going to be showing as an example today actually come from *real* customers from my very *real* website. Don´t worry, these messages are completely anonymized and do not contain any sensitive information. The website helps English teacher find easy classroom material, and everyday teachers write to us directly asking for specific material. Now in my case, each request we receive falls into one of two categories, it is either a request that can be dealt with automatically (category *0*) or it is something more detailed and complex and it needs manual,human intervention (category *1*). For your own case, the categories might vary, you could urgent and non-urgent help requests, or even classify them into seperate area ( category 0 messages for general support, category 1 messages goes to the sales teams, category 2 messages are for the fraud team, etc...). All depends on your needs. \n",
        "\n",
        "The idea here is for you to get an idea of how we can use A.I and M.L on raw text communication, so that it can inspire you to automate other parts of your company ( email classification, chatbots, presentation summarizers, the list is endless.)\n",
        "\n",
        "The great thing for our use case is that the data is extremely basic: We just need the raw message from our message and its associated label. Have a look here at the [example dataset that we´ll be using here](https://docs.google.com/spreadsheets/d/1cbFni_iqYCNzQOHLjAfFwCKViU05nuSXSbrVeCcnf2U/edit#gid=0) from my users. So you can just scrape through all your pass customer service requests that will be enough for you to automate for the future.\n",
        "\n",
        "So here is, as always, the order of proceedings to get this mission accomplished:\n",
        "\n",
        "##Part 1: Data Cleaning ( As always the most important part)\n",
        "\n",
        "##Part 2 : Data Learning ( As always, the surprisingly easiest part)\n",
        "\n",
        "##Part 3: Data Predicting (As always the most fun part if all goes well)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "5_0ZyeEJRBLB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 0- Importing libraries\n",
        "\n",
        "But before all that, let´s import our libraries without which we couldn´t perform any magic tricks."
      ]
    },
    {
      "metadata": {
        "id": "vxSIfG6INDaW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A general purpose ML library that has almost everthing we need\n",
        "import sklearn\n",
        "from  sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline, make_union\n",
        "from sklearn.svm import LinearSVC\n",
        "# for making linear algebra readable\n",
        "import numpy as np\n",
        "\n",
        "#Allows to upload and download files directly from the browser\n",
        "from google.colab import files \n",
        "\n",
        "\n",
        "#Allows us to manipulate data in a quick and easy way\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bq3s4iE0R1NR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1- Data Cleaning\n",
        "\n",
        "So let´s get started with the data pre-processing or \"cleaning\". This first step, before any actual Machine Learning has been done, is actually the most crucial. Think of it as cooking a meal, even if we have the best recipe in the world, if our ingredients are going off or are missing, then our food will not taste well no matter what fancy utensils you use. On the flip side, if you have fresh, high quality ingredients, you can whip up a pretty delicious meal with a pretty simple recipe.\n",
        "\n",
        "The same analogy applies to M.L, if you don´t have well processed data, then you ca forget about the rest. First of all, let´s have a look at our dataset. As I mentioned before, we start of with two columns, the raw text input, and the second column giving us its category, being either 0 as a low level request, and 1 being an urgent query that requires human intervention: "
      ]
    },
    {
      "metadata": {
        "id": "VufEOWF-iilo",
        "colab_type": "code",
        "outputId": "587e55df-3348-4200-b38e-4b644e76c52f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "cell_type": "code",
      "source": [
        "data_url=(\"https://raw.githubusercontent.com/busyML/Customer-Queries-Classification-NLP/master/NLP%20INSTANTEACH%20ML%20-%20Sheet1.csv\")\n",
        "\n",
        "data=pd.read_csv(data_url)\n",
        "\n",
        "data.head(11)\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input Query</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>be or do</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>comparatives and superlatives</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>how to teach overcoming obstacles concept wit...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Present perfect simple or continuous</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-----</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1st conditional</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2018 World Cup</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2018 World Cup</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2nd conditional</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>A career profile\\nA descriptive article\\nInfor...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Input Query  Category\n",
              "0                                            be or do         0\n",
              "1                       comparatives and superlatives         0\n",
              "2    how to teach overcoming obstacles concept wit...         1\n",
              "3                Present perfect simple or continuous         0\n",
              "4                                                   -         0\n",
              "5                                               -----         0\n",
              "6                                     1st conditional         0\n",
              "7                                      2018 World Cup         1\n",
              "8                                      2018 World Cup         1\n",
              "9                                     2nd conditional         0\n",
              "10  A career profile\\nA descriptive article\\nInfor...         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "vL9R-2HVIRK6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Always Shuffle\n",
        "\n",
        "So as we seen in other tutorials,  shuffling our data randomly should always be our first step. In our case, our data is ordered alphabetically, so we don´t want that to cause any unwanted biases to slip. Its always safest to just shuffling our data before we even start touching. And there is noe excuse for not doing so when it can be done in 1 line of code: "
      ]
    },
    {
      "metadata": {
        "id": "NJCRYcaJ66SY",
        "colab_type": "code",
        "outputId": "cf1fab2a-cf3f-41ff-f469-912b14b22908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#we use the \"sample\" command of pandas to shuffle our data, the random state means that we will always shuffle the data in the same way so that when different people load this code, they will all get the same results.\n",
        "data= data.sample(frac=1, random_state=11)\n",
        "\n",
        "#we print out the first 11 rows of our data to check that it has indeed been shuffled, on the left we have the index number which we can also think of as an ID number.\n",
        "\n",
        "data.head(11)\n",
        "\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input Query</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>690</th>\n",
              "      <td>Simple present, readings, songs, hobbies, food...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>940</th>\n",
              "      <td>slavery in the US</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>Passive voice</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>Current affairs</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>456</th>\n",
              "      <td>Modal verbs</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751</th>\n",
              "      <td>sport and cities of the world</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>Grammar</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>FIRST LESSON</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>Art or travelling</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>824</th>\n",
              "      <td>Tv series</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>559</th>\n",
              "      <td>Practice different uses of can and could (abil...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Input Query  Category\n",
              "690  Simple present, readings, songs, hobbies, food...         0\n",
              "940                                  slavery in the US         1\n",
              "503                                      Passive voice         0\n",
              "170                                    Current affairs         1\n",
              "456                                        Modal verbs         0\n",
              "751                      sport and cities of the world         1\n",
              "311                                            Grammar         0\n",
              "999                                       FIRST LESSON         0\n",
              "57                                   Art or travelling         0\n",
              "824                                          Tv series         0\n",
              "559  Practice different uses of can and could (abil...         0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "vQi5dV_2q1GI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#data.to_csv(\"shuffled.csv\")\n",
        "\n",
        "#files.download(\"shuffled.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "30Pq9iCyjSTA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 1- Vectorization of Words\n",
        "\n",
        "So there is a clear issue that you are probably wondering about, which is of course, how on earth can an algorithm read text. Well, the answer is, it doesn´t. As is alwasy the case, we are going to have find a way to convert our raw text to numbers so that our algorithm can learn from.\n",
        "\n",
        "Fortunately, this is a well studied topic and there are countless of options we could use, ranging from the simplest to the most insanely complex.\n",
        "We´ll go through the simplest form, explain its limitation and then gives us a medium range solution that should work in most cases:\n",
        "\n",
        "***Counting Word Frequency***\n",
        "\n",
        "So this is the graddad of word vectorization, the simplest way we can transform words to number, for each word in a sentence, we count the number of times it features. When we are comparing different raw text, we´ll take the whole range of vocabulary and count for each sentence the frequency of for each word of our vocabulary. This simplistic technique is commonly called the \"bag of words\" technique\n",
        "\n",
        "![alt text](https://www.python-course.eu/images/bag_of_words.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "To see this in practice, [here is how the dataset would look after the transformation at the following link](https://docs.google.com/spreadsheets/d/1diEXKKl8j4SsqEopc7ln1NTZNzUZ6oFcN371XSkEoWo/edit#gid=1530603417). \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pOHbx4KqmMWD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "count_vectorization = CountVectorizer()\n",
        "\n",
        "count_example= (count_vectorization.fit_transform(data[\"Input Query\"].values.astype('U'))).toarray()\n",
        "\n",
        "count_example =pd.DataFrame(count_example)\n",
        "\n",
        "vocab_list=list (count_vectorization.get_feature_names())\n",
        "\n",
        "i=0\n",
        "for i in range(len(count_example.columns)):\n",
        "  count_example.rename(columns={i: vocab_list[i]}, inplace=True)\n",
        "  \n",
        "\n",
        "#count_example.to_csv(\"countexample.csv\")\n",
        "\n",
        "#files.download(\"countexample.csv\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b_sn2IXTszBT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "However, this way of converting the text really isn´t the best, sometimes the algorithm won´t be able to learn much from it because of its simplistic nature. But most importantly, this count vectorization generates a big big dataset and this can be a big problem. Take for instance our small dataset here is only of a thousand queries and there are 1,200 unique vocabulary words. 1000 x 1,200 = .... Over a million cells! So even with just a small dataset of queries, our numerical dataset is huge. Now imagine that a medium sized company will probably have 100,000 past examples with 10,000 of individuals and then the size truly becomes mind boggling. We need a more practical system that can scale better.\n",
        "\n",
        "**Hashing Vectorization**\n",
        "\n",
        "So hashing vectorization uses a bit of good´ol math wizardy to plot all the words on a graph and then evaluates their proximity to another. This way, the algorithm can start understanding the relevance of one word to another and get a better overall idea of how some words are interconnected. To wrap your head around this concept, have a look at the following representations:\n",
        "\n",
        "![alt text](https://www.tensorflow.org/images/linear-relationships.png)\n",
        "\n",
        "![alt text](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/08/Scatter-Plot-of-PCA-Projection-of-Word2Vec-Model.png)\n",
        "\n",
        "As we can see, the algorithms looks at how words tend to cluster in different contexts and finds the relations between them based on how they were used in the text. Once, again [if you want to see how the dataset ended up looking, have a gander over at this link.](https://docs.google.com/spreadsheets/d/1diEXKKl8j4SsqEopc7ln1NTZNzUZ6oFcN371XSkEoWo/edit#gid=281324509)"
      ]
    },
    {
      "metadata": {
        "id": "gjTwMl84lQoE",
        "colab_type": "code",
        "outputId": "efebb308-bc6c-4766-ba82-9d09783edec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "cell_type": "code",
      "source": [
        "#We call the Hashing Vectorizer from SKlearn, and we limit its columns to 1024 (2 to the tenth power)\n",
        "\n",
        "vectorization= HashingVectorizer(n_features=2**10, norm = \"l1\")\n",
        "\n",
        "#applying the vectorizer on our text\n",
        "vec_counts = (vectorization.fit_transform(data[\"Input Query\"].values.astype('U'))).toarray()\n",
        "\n",
        "#putting our training data in Pandas format\n",
        "training_data=pd.DataFrame(vec_counts)\n",
        "\n",
        "#We create an excel file that contains the wine with their new categories\n",
        "#training_data.to_excel(\"instanteachnlptraining.xlsx\")\n",
        "\n",
        "#We use the \".download\" command to download the new excel file to our browser\n",
        "#files.download(\"instanteachnlptraining.xlsx\")\n",
        "\n",
        "training_data.head(5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>1014</th>\n",
              "      <th>1015</th>\n",
              "      <th>1016</th>\n",
              "      <th>1017</th>\n",
              "      <th>1018</th>\n",
              "      <th>1019</th>\n",
              "      <th>1020</th>\n",
              "      <th>1021</th>\n",
              "      <th>1022</th>\n",
              "      <th>1023</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1024 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0     1     2     3     4     5     6     7     8     9     ...   1014  \\\n",
              "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
              "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
              "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
              "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
              "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...    0.0   \n",
              "\n",
              "   1015  1016  1017  1018  1019  1020  1021  1022  1023  \n",
              "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
              "\n",
              "[5 rows x 1024 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "sinwqPmp0_qd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Right, so after a few lines of code, we have our dataset. Most of the time when dealing with text input, this Hash vectorization will be the way to go. The great thing here is that we are able to limit the number of columns of the conversion meaning that our dataset is limited in size, it will always have around 1,000 columns, which means that adding more examples is less problematic. If you wish to use this code for yourself and you want to use something closer to 100,000 examples rather than a 1,000 like I have, you might want to increase the \"n_features\" to 2^11 or 2^12.\n",
        "\n",
        "However, although our dataset is now no longer going to grow expontionally, it is still rather big and maybe still be difficult to scale depending on your ressources. Furthermore if you look at our dataset, you´ll see that there are many many zeroes, which seems a bit unnecessary. If only there was a way to compress all these numbers so that it didn´t take so much space in our computer´s memory...\n",
        "\n",
        "**Compressing the data with PCA**\n",
        "\n",
        "Well, what we can now do is to compress our data! There is a great technique for this called [Principal Component Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), or PCA as every normal human being calls it. PCA takes any dataset and basically boils it down to a more concise version, keeping only the most essential data and discarding the rest, allowing the dataset to become smaller. It´s like when you take someone rambling 3 paragraph email and turned it into a few bullet points. Or another analogy could be of taking a high resolution picture and converting it to a lower quality; we´ll still be able to see and understand the picture despite it being a bit fuzzier, but the picture will take up less space: \n",
        "\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/Benli11/data/master/img/reTiger.png)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "E92V8CcJ0-py",
        "colab_type": "code",
        "outputId": "c9c06b10-eb1d-421f-f241-d0ac57be01e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "cell_type": "code",
      "source": [
        "training_rows, training_columns= training_data.shape\n",
        "\n",
        "#We load PCA from Sklearn, the \"0.99\" means that we want the compression to retain 99% of the original data´s significance. \n",
        "pca_compressor=PCA(0.99)\n",
        "\n",
        "#We use the PCA transformer to compress the image and we use the \"DataFrame\" function to convert it into our favorite pandas format\n",
        "compressed_training_data = pd.DataFrame(pca_compressor.fit_transform(training_data))\n",
        "\n",
        "#we get how many columns the new compressed dataset has so that we can compate with the original one.\n",
        "compressed_rows, compressed_columns = compressed_training_data.shape\n",
        "\n",
        "#We  print out the comparison between the two\n",
        "print(\"number of columns before compression:\",training_columns,\"\\n\",\"number of columns after compression:\" ,compressed_columns)\n",
        "\n",
        "#compressed_training_data.to_excel(\"pcadata.xlsx\")\n",
        "\n",
        "#files.download(\"pcadata.xlsx\")\n",
        "\n",
        "compressed_training_data.head()\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of columns before compression: 1024 \n",
            " number of columns after compression: 369\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>359</th>\n",
              "      <th>360</th>\n",
              "      <th>361</th>\n",
              "      <th>362</th>\n",
              "      <th>363</th>\n",
              "      <th>364</th>\n",
              "      <th>365</th>\n",
              "      <th>366</th>\n",
              "      <th>367</th>\n",
              "      <th>368</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.033129</td>\n",
              "      <td>-0.031765</td>\n",
              "      <td>0.041626</td>\n",
              "      <td>0.004632</td>\n",
              "      <td>0.001425</td>\n",
              "      <td>-0.037356</td>\n",
              "      <td>0.016783</td>\n",
              "      <td>0.004611</td>\n",
              "      <td>0.024837</td>\n",
              "      <td>-0.018390</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.001405</td>\n",
              "      <td>0.002806</td>\n",
              "      <td>0.002634</td>\n",
              "      <td>0.000792</td>\n",
              "      <td>-0.002734</td>\n",
              "      <td>-0.009453</td>\n",
              "      <td>-0.008287</td>\n",
              "      <td>0.006991</td>\n",
              "      <td>0.009667</td>\n",
              "      <td>0.008540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.018414</td>\n",
              "      <td>-0.004113</td>\n",
              "      <td>-0.032515</td>\n",
              "      <td>-0.010081</td>\n",
              "      <td>0.024245</td>\n",
              "      <td>-0.026045</td>\n",
              "      <td>-0.007159</td>\n",
              "      <td>-0.009223</td>\n",
              "      <td>-0.006876</td>\n",
              "      <td>-0.015664</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003613</td>\n",
              "      <td>0.002436</td>\n",
              "      <td>0.004473</td>\n",
              "      <td>0.002216</td>\n",
              "      <td>0.009240</td>\n",
              "      <td>0.010201</td>\n",
              "      <td>0.003101</td>\n",
              "      <td>-0.005442</td>\n",
              "      <td>0.013432</td>\n",
              "      <td>0.000622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.026933</td>\n",
              "      <td>-0.012493</td>\n",
              "      <td>-0.056835</td>\n",
              "      <td>-0.024817</td>\n",
              "      <td>0.064456</td>\n",
              "      <td>-0.060235</td>\n",
              "      <td>0.022895</td>\n",
              "      <td>-0.035159</td>\n",
              "      <td>-0.127569</td>\n",
              "      <td>-0.051734</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001314</td>\n",
              "      <td>-0.000214</td>\n",
              "      <td>0.000891</td>\n",
              "      <td>0.002236</td>\n",
              "      <td>0.003450</td>\n",
              "      <td>-0.002849</td>\n",
              "      <td>-0.004250</td>\n",
              "      <td>0.003556</td>\n",
              "      <td>-0.004682</td>\n",
              "      <td>0.001619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.016188</td>\n",
              "      <td>-0.005682</td>\n",
              "      <td>-0.028671</td>\n",
              "      <td>-0.009872</td>\n",
              "      <td>0.019837</td>\n",
              "      <td>-0.020642</td>\n",
              "      <td>-0.008372</td>\n",
              "      <td>-0.007497</td>\n",
              "      <td>-0.003220</td>\n",
              "      <td>-0.012359</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.001933</td>\n",
              "      <td>0.002523</td>\n",
              "      <td>0.001870</td>\n",
              "      <td>0.003681</td>\n",
              "      <td>-0.003843</td>\n",
              "      <td>-0.009569</td>\n",
              "      <td>-0.002016</td>\n",
              "      <td>0.007450</td>\n",
              "      <td>0.014070</td>\n",
              "      <td>-0.001530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.036033</td>\n",
              "      <td>-0.025344</td>\n",
              "      <td>-0.130854</td>\n",
              "      <td>-0.379420</td>\n",
              "      <td>-0.409186</td>\n",
              "      <td>0.109595</td>\n",
              "      <td>0.002428</td>\n",
              "      <td>0.008506</td>\n",
              "      <td>-0.008649</td>\n",
              "      <td>0.023388</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003951</td>\n",
              "      <td>0.003319</td>\n",
              "      <td>-0.006984</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>0.014244</td>\n",
              "      <td>0.002407</td>\n",
              "      <td>-0.001789</td>\n",
              "      <td>0.007050</td>\n",
              "      <td>-0.000935</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 369 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4         5         6    \\\n",
              "0 -0.033129 -0.031765  0.041626  0.004632  0.001425 -0.037356  0.016783   \n",
              "1 -0.018414 -0.004113 -0.032515 -0.010081  0.024245 -0.026045 -0.007159   \n",
              "2 -0.026933 -0.012493 -0.056835 -0.024817  0.064456 -0.060235  0.022895   \n",
              "3 -0.016188 -0.005682 -0.028671 -0.009872  0.019837 -0.020642 -0.008372   \n",
              "4 -0.036033 -0.025344 -0.130854 -0.379420 -0.409186  0.109595  0.002428   \n",
              "\n",
              "        7         8         9      ...          359       360       361  \\\n",
              "0  0.004611  0.024837 -0.018390    ...    -0.001405  0.002806  0.002634   \n",
              "1 -0.009223 -0.006876 -0.015664    ...     0.003613  0.002436  0.004473   \n",
              "2 -0.035159 -0.127569 -0.051734    ...     0.001314 -0.000214  0.000891   \n",
              "3 -0.007497 -0.003220 -0.012359    ...    -0.001933  0.002523  0.001870   \n",
              "4  0.008506 -0.008649  0.023388    ...    -0.003951  0.003319 -0.006984   \n",
              "\n",
              "        362       363       364       365       366       367       368  \n",
              "0  0.000792 -0.002734 -0.009453 -0.008287  0.006991  0.009667  0.008540  \n",
              "1  0.002216  0.009240  0.010201  0.003101 -0.005442  0.013432  0.000622  \n",
              "2  0.002236  0.003450 -0.002849 -0.004250  0.003556 -0.004682  0.001619  \n",
              "3  0.003681 -0.003843 -0.009569 -0.002016  0.007450  0.014070 -0.001530  \n",
              "4  0.001156  0.014244  0.002407 -0.001789  0.007050 -0.000935  0.000010  \n",
              "\n",
              "[5 rows x 369 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "SYGxtra87_U6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Well would you believe it, after appying the compression process, we only have 369 columns in our dataset, in other words we´ve reduced the size of the dataset by  65% all the way only losing 1% of the original data. As you can se, we no longer have sparse zero columns, everything has been compressed as tightly as possible in order to have as few columns as possible. This now makes our data far more scalable as we add future examples to it.\n",
        "\n",
        "### The \"answer\" dataset\n",
        "\n",
        "Well we now have our training data ready, all we need now is to create another, much smaller dataset that stores the attributed label, we can do so with one line of code. Our algorithm will use this as its examples to learn to classify future queries."
      ]
    },
    {
      "metadata": {
        "id": "iumUSagmwyCV",
        "colab_type": "code",
        "outputId": "aaeb7c46-6356-4a0d-e0bf-93a4ee6903e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "cell_type": "code",
      "source": [
        "training_answers= data[\"Category\"]\n",
        "\n",
        "training_answers.head(15)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "690    0\n",
              "940    1\n",
              "503    0\n",
              "170    1\n",
              "456    0\n",
              "751    1\n",
              "311    0\n",
              "999    0\n",
              "57     0\n",
              "824    0\n",
              "559    0\n",
              "207    0\n",
              "919    1\n",
              "833    0\n",
              "530    0\n",
              "Name: Category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "jIkhOwiT2y0p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our model will use these answers to \"learn\" the relation between each input text and it´s label category.\n",
        "\n",
        "Of course, in your context, you might have several different categories assigned to your customer queries. The models should be able to handle multi queries as well if you so need it. "
      ]
    },
    {
      "metadata": {
        "id": "6dN2d34A3X8Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Learning from the data\n",
        "\n",
        "So now the \" surprisingly easy\" part. Indeed, if our data has been well cleaned and formatted in the previous step, it should be relatively easy to get good results. First of all, this is classification problem, so the first thing that should jump to our mind is : what more dangerous for my company? A false positive or a false negative ? Enter the concepts of Precision and Recall:\n",
        "\n",
        "###Step 1- Choose between Precision or Recall as our key metric:\n",
        "\n",
        "First of all, let's define some useful vocabulary. This will help us to understand how well our model is performing. There are four types of predictions that our model could make: True Positive, False Positive, True Negative, False Negative. Let's define these terms:\n",
        "\n",
        "True Positive: This is a good prediction, our model predicted that the customer was going to churn (emitted a 1) and the customer did in reality churn (churn=1).\n",
        "\n",
        "False Positive: This is an incorrect prediction, our model predicted that our customer churned ( emitted a 1), however it turns out that customer did not in fact churn (churn=0)\n",
        "\n",
        "True Negative : This is again a correct prediction, our model said that the customer was not going to churn ( it emitted a 0) and this turned out to be correct in the real world ( churn was equal to 0)\n",
        "\n",
        "False Negative: Again, this is an incorrect prediction, but of a different kind. Here our model predicted that the customer was not going to churn (emitted a 0), but lo and behold, in reality our customer did in fact leave the company (churn=1)\n",
        "\n",
        "With this now clearly defined, we need to decide what evaluation metric we will use to evaluate our model.\n",
        "\n",
        "This actually is a crucial concept, and we have three evaluation options to choose from:\n",
        "\n",
        "\n",
        "*   **Precision** - High precision means that the model does not give many \"False Positives\".\n",
        "\n",
        "*   **Recall** High Recall means that model has a very low proportion of \"False Negatives\".\n",
        "\n",
        "* **F1 Score **  The F1 Score is a balance between Precision and Recall, this a great measure for overall accuracy.\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png)\n",
        "\n",
        "\n",
        "Which of these metrics are most important to pay attention to? Well, this wholly depends on your context and what type of error is most costly to your business. Here we need to analyse the consequence of each type of error.\n",
        "\n",
        "If, for example a false positive is very very costly, such as in the case of Face Recognition or Mortgage Approval, then we will want to maximise the Precision metric as much as possible.\n",
        "\n",
        "However, if getting a false negative is even more problematic, as for example in cases such as Fraud Detection or Cancer Diagnosis, then we'll make sure our model has the highest Recall measure possible.\n",
        "\n",
        "If obtaining a false negative is just as bad as a false positive and there is no difference between them, then we can simply use the F1 score.\n",
        "\n",
        "Remember there is no right or wrong answer here because it is a judgment call based on your own individual context.\n",
        "\n",
        "So for the personal context of my website, it just so happens that it is far more important to be able to detect the \"1\" category because these are urgent and need human intervention, whereas the \"0\" category responses are not so important since they can be dealt with automatically by the system. It´s not a big deal if a few \"0\" type queries get bunched with the \"1\", a human can also deal with them. Howeve, if a user is issuing an urgent query that demands human attention and the model classifies that as being a non-important \"0\" query, then that could cause high customer unsatisfaction ( because no one will ever answer him/her).\n",
        "\n",
        "Therefore in practice, I will want by model to be \"paranoid\" and to always lean towards the \"1\" category unless it is extremely certain that it is a type \"0\" query. Basically, if there is any doubt, the model should classify the query as being of a \"1\" just to be on the safe side of things.\n",
        "\n",
        "Of course this decision is based on my unique business context, you will have to go through this decision making process yourself for your own company needs, but it´s nothing that a bit of common sense can´t handle.\n",
        "\n",
        "So how do we manage this programtically? We simply need to assign to each category, in our case \"0\" and \"1\", different weights. We´ll assign the a heavier weight to the \"1\" category because we want to be as certain as possible to always detect that class. If you want to use this code for yourself, you can adjust the weights for your own classes to what you need for your own use case.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3RlwmRQXOXKL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#class weights tells the algorithm how paranoid to be about each class. In this case, the 0.87 for the \"1\" category means that it will only categorize a query as \"0\" if it is more than 87% sure. Otherwise it will classify it as \"1\", which means that it´s not taking any risks with the \"1\" category\n",
        "class_weights = {0:0.13, 1:0.87}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5WtUwDprWnL8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 2- Learning about the learning algorithm\n",
        "\n",
        "Right, now let´s get to the fancy part. We´ll be using an algorithm called a linear SVC ( Support Vector Classifier). It´s not too complex of a concept as I hope you can discern from the beautiful drawings below. And as always, you don´t need to know the math, all you need to know is the one line of code to load from the SKlearn library.\n",
        "\n",
        "![alt text](http://michelleful.github.io/code-blog/assets/images/201506/svm2_new.png)\n",
        "\n",
        "![alt text](https://chrisalbon.com/images/machine_learning_flashcards/Support_Vector_Classifier_print.png)\n",
        "\n",
        "As always we can load the model with one simple line of code and train it with the \".fit\" command to get to learn from the data. Learning from the data will only take a few seconds! As you can see from the code below, it is very easy and simple to implement: \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AhX7S17kja_O",
        "colab_type": "code",
        "outputId": "9385df51-59e0-4bb2-9a98-3fb013935859",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# We load the LinearSVC from Scikit learn. We use the C parameter from stopping the model jsut memorizing the dataset, then we also input the clas weights... in the class weights parameter\n",
        "svc_model= LinearSVC(C=7, dual=True, loss=\"squared_hinge\", penalty=\"l2\", tol=1e-15, class_weight=class_weights)\n",
        "\n",
        "#we use the .fit command to get the model to learn from the formatted compressed data, matching them with the training answers. \n",
        "svc_model.fit(compressed_training_data,training_answers)\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=7, class_weight={0: 0.13, 1: 0.87}, dual=True, fit_intercept=True,\n",
              "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "     multi_class='ovr', penalty='l2', random_state=None, tol=1e-15,\n",
              "     verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "hRgRDaq_RlWA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 3 - Evaluating the model\n",
        "\n",
        "So now we actually need to test our model, to see if this thing actually works! Below I´ve contructed a little program that counts the number of false positives and the number of false negatives etc. \n",
        "\n",
        "For the eagle eyed out there, you will have noticed that we are not using \"test\" dataset to examinate the performance of our model, but rather, we are re-using out training data for this. How dare we?! The reason is that we are in the online context, where this model will always be tested and learning in real time as user input comes through. The true test will come when we deploy into the real world. This also we´ll be adding to our dataset new examples every couple of weeks ideally and that it will steadily grow over time. \n",
        "\n",
        "So below we´ll count the number of correct and inccrrect answers and we´ll print our our key metrics: \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "SrhZlAuVj5XN",
        "colab_type": "code",
        "outputId": "ebe1c67a-d593-4c8f-ca3e-8e9719b91c91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "#initializing our counts of true positive, false negative, etc.\n",
        "TP=0\n",
        "TN=0\n",
        "FP=0\n",
        "FN=0\n",
        "\n",
        "#we create what is called a for loop to iterate through every row of the dataset\n",
        "for i in range(len(training_data)):\n",
        "    #Counts True Positives if the answer is \"1\" and the model predicted \"1\"\n",
        "    if svc_model.predict(compressed_training_data.iloc[[i,]])==1 and training_answers.iloc[i]==1:\n",
        "                                       TP=TP+1\n",
        "        \n",
        "    ##Counts False Positives if the answer is \"0\" and the model predicted \"1\"\n",
        "    if svc_model.predict(compressed_training_data.iloc[[i,]])==1 and training_answers.iloc[i]==0:\n",
        "                                       FP=FP+1    \n",
        "    ##Counts True Negatives if the answer is \"0\" and the model predicted \"0\"\n",
        "    if svc_model.predict(compressed_training_data.iloc[[i,]])==0 and training_answers.iloc[i]==0:\n",
        "                                       TN=TN+1    \n",
        "    \n",
        "    #Counts False Negatives if the answer is \"1\" and the model predicted \"0\"\n",
        "    if svc_model.predict(compressed_training_data.iloc[[i,]])==0 and training_answers.iloc[i]==1:\n",
        "                                       FN=FN+1   \n",
        "\n",
        "print (\"Model\", \"True Positives:\",TP, \"False Positives:\",FP,\"True Negatives:\",TN , \"False Negatives:\", FN) \n",
        "\n",
        "model_accuracy= (TP+TN)/(len(training_answers))\n",
        "model_precision= TP/(TP+FP) \n",
        "model_recall=TP/(TP+FN)\n",
        "model_f1= 2 * (model_precision * model_recall) / (model_precision + model_recall)\n",
        "\n",
        "print(\"Model´s Accuracy On Training Data:\", model_accuracy *100,'%')\n",
        "print(\"Model´s Precision On Training Data:\", model_precision *100,'%')\n",
        "print(\"Model´s Recall On Training Data:\", model_recall *100,'%')\n",
        "print(\"Model´s F1 On Training Data:\", model_f1 *100,'%')\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model True Positives: 187 False Positives: 125 True Negatives: 762 False Negatives: 1\n",
            "Model´s Accuracy On Training Data: 88.27906976744187 %\n",
            "Model´s Precision On Training Data: 59.93589743589743 %\n",
            "Model´s Recall On Training Data: 99.46808510638297 %\n",
            "Model´s F1 On Training Data: 74.79999999999998 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "48JTORkPsTyO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So we´ve calculated our metrics by using some pretty simple division and multiplication. \n",
        "\n",
        "And as we can see, the most important metric for us was pretty good, with 99% ( note that this will problably drop in the real world to maybe around 90%). We also se that we have quite a few false negatives but that´s ok for our business context. Of course, if if in your case you wanted to prioritize the \"0\" category, then the **precision** metric is the one you would try to maximize ( by readjusting the class weights from earlier.)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4PDK-vTHtxOh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 3- Data Predicting \n",
        "\n",
        "So we've done the hardest part, now we need to create a little program that allows us to classify any text inputed into it. The ida here is that you'll have this program waiting the background on your platform and whenever a customer issues a customer support query, then the program will classify query and direct to correct funnel for it to be solved. \n",
        "\n",
        "This section is more to do with dev implementation and deployment, so it's not so important to look through the code. However, I do recommend that you have a go at it yourself and input some text as an example. (For this, pretend that you are an english teacher who needs a specific topic for your class. A general query should output a 0 whereas a specific query should be classified as a 1.) Have fun!"
      ]
    },
    {
      "metadata": {
        "id": "CggYEIW6kbSH",
        "colab_type": "code",
        "outputId": "d9b62864-0516-4345-ba81-592f71227cfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "def query_classifier(new_input):\n",
        "  #we get the raw text into a \"list\" format with these brackets\n",
        "  new_input=[new_input]\n",
        "\n",
        "  #Now we need to format it in the same way we formated our training data. we first apply to it the hash vectorization to get it into the same format\n",
        "  new_input_vectorized = vectorization.fit_transform(new_input)\n",
        "  new_input_vectorized=pd.DataFrame(new_input_vectorized.toarray())\n",
        "  \n",
        "  #Now that we have the hash vectors, we can compress it using PCA (we actually need to add to the training set because PCA compresses the data in function of other data)\n",
        "  compressing_new_input= training_data.append(new_input_vectorized, ignore_index=True)\n",
        "  pca_input_compressor= PCA(n_components=compressed_columns, svd_solver='full')\n",
        "  \n",
        "  #We compress the data...\n",
        "  compressing_new_input= pd.DataFrame(pca_input_compressor.fit_transform(compressing_new_input))\n",
        "  \n",
        "  #And now we extract the last row that corresponds to the last row which is our new formatted input that we want to predict\n",
        "  new_input_compressed = compressing_new_input.iloc[[(len(compressing_new_input.index)-1),]]\n",
        "\n",
        "  #Now we use the \".predict\" function to classify the text as \"0\" or \"1\"\n",
        "  prediction=svc_model.predict(new_input_compressed)\n",
        "  \n",
        "  if prediction==0:\n",
        "    print(prediction)\n",
        "    print(\"Not to worry, we can deal with this query automatically. This is not an urgent request!\")\n",
        "  else:\n",
        "    print(prediction)\n",
        "    print(\"Human, please help! This request is too complex and specific... Please do it manually\")\n",
        "    \n",
        "\n",
        "new_input= (input(\"input new text:\"))\n",
        "\n",
        "\n",
        "query_classifier(new_input)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input new text:Future perfect\n",
            "[0]\n",
            "Not to worry, we can deal with this query automatically. This is not an urgent request!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MTEJBEnMEylW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "That's all. I hope you were able to appraciate how easy it was to automatic a significant portion of the customer support query help line. Once we know what type of query is being inputted, we can do lots of magical things like take the user to the exact help page he/she needs or open a chat with a human agent all depending on what they wrote in their support query and the problem we are having. I can only see this as a win win seeing as the user gets a more personalized experience meanwhile the company saves precious human time.\n",
        "\n",
        "So that will be it for today. The key things to remember is that by using a \"has vectorizer\", we can conveniently convert any raw text to numbers that can then be easily digested by the algorithm. However, depending on your case, this text to number conversion can generate huge datasets, so we can use PCA compression to reduce it and save valuable memory space.\n",
        "\n",
        "Lastly and perhaps the most important message of all is that I hope you continue to be convinced that you don't need to be a programmer to implement these tools in your company today and you can do so to make your and your coleagues' lives a lot easier.\n",
        "\n",
        "Feel free to contact me about any questions, comments or feedback at my email: [conrad.w.s@gmail.com](mailto:conrad.w.s@gmail.com) or hit me up [on Linkedin at Conrad WS.](https://www.linkedin.com/in/conrad-wilkinson-schwarz-210aa9b2/)"
      ]
    },
    {
      "metadata": {
        "id": "GDSYx7VSGjIF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "95df868e-6bbc-47c4-8b32-7a29f5668aa1"
      },
      "cell_type": "code",
      "source": [
        "print(\"thank you for reading!!\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "thanks for reading!!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}